<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Stolen from Sergey and Jon Barron */
        /* with significant help from Debidatta Dwibedi's webpage */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;


            font-size: 14px
        }

        strongred {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            color: red;
            font-size: 14px
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }
    </style>
    <!-- <link rel="icon" type="image/png" href="seal_icon.png"> -->
    <script type="text/javascript" src="hidebib.js"></script>
    <title>Jyun-Ting Song</title>
    <meta name="Jyun-Ting&#39;s CMU Homepage" http-equiv="Content-Type"
          content="Jyun-Ting&#39;s Homepage">

    <link href="stylesheet.css" rel="stylesheet" type="text/css">
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-90857215-1', 'auto');
        ga('send', 'pageview');

    </script>
</head>

  <body>
    <script>
      function showEmail() {
        document.getElementById('emailHolder').innerHTML = 'ude.umc.werdna@sniutnyj'.split("").reverse().join("");
      }
    </script>

    <table width="960" border="0" align="center" cellspacing="0" cellpadding="20">
        <tbody>
        <tr>
            <td>
                <p align="center"><font size="7">Jyun-Ting Song</font><br>
                </p>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                    <tr>
                        <td width="67%" valign="middle" align="justify">
    
                          <p>
                            I am a second-year master's student at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, 
                            advised by Prof. <a href="https://kriskitani.github.io/">Kris Kitani</a>.
                            Previously, I earned my B.S. in Electrical Engineering from <a href="https://en.ntnu.edu.tw/">National Taiwan Normal University</a>, 
                            where I was advised by Prof. <a href="https://sites.google.com/site/jackybaltes/home">Jacky Baltes</a>.
                            <br>
                            <br>
                            My research interest lies in the intersection of Computer Vision, Machine Learning and Robotics.
                            Currently, I am working on human pose estimation and human-object contact detection.
                            Previously, I have worked on humanoid control in both simulation and real-world robotics.
                            <br>
                            <br>
                            I love humans, and I am interested in understanding them. I hope my reseach can contribute to this direction.
                            <br>
                            <br>

                          </p>
                          
                          <p style="text-align:center">
                            <a href="https://jyuntins.github.io/#" onclick="showEmail()" id="emailHolder"> Click to Reveal Email </a> &nbsp;/&nbsp;
                            <a href="data/Jyun-Ting-CV.pdf">CV</a> &nbsp;/&nbsp;
                            <a href="https://scholar.google.com/citations?hl=en&user=N1dz8cAAAAAJ">Scholar</a> &nbsp;/&nbsp;
                            <a href="https://github.com/jyuntins/">GitHub</a> &nbsp;/&nbsp;
                            <a href="https://www.linkedin.com/in/jyun-ting-song-699b91222/">Linkedin</a>
                          </p>

                        </td>
                        <td style="padding:2.5%;width:40%;max-width:40%">
                          <a href="images/jyuntins.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/jyuntins.jpg" class="hoverZoomLink"></a>
                        </td>
                    </tr>
                    </tbody>
                </table>
    
                <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    
    
                    <tbody>
                    <tr>
                        <td>
                            <h2>Research</h2>
                            My research interest lies in the intersection of Computer Vision, Machine Learning and Robotics.
                            Currently, I am working on human pose estimation and human-object contact detection.
                            Previously, I have worked on humanoid control in both simulation and real-world robotics.
                            <br>
                            <br>
                            I love humans, and I am interested in understanding them. I hope my reseach can contribute to this direction.

    
                        </td>
                    </tr>
    
                    </tbody>
                </table> -->
    
    
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    
                  <tbody>
                    <tr>
                        <td>
                            <h2 id="publications">Publications</h2>
                        </td>
                    </tr>
                  
                    <tr>
                        <td style="padding:20px;width:20%;vertical-align:middle">
                          <video  width=100% muted autoplay loop>
                            <source src="images/contact4d.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                          </video>
                        </td>

                        <td width="58%" valign="top">
                            <p><a href="https://jyuntins.github.io/harmony4d" id="contact4d_title">
                                <heading>Contact4D: A Video Dataset for Whole-body Human Motion and Finger Contact in Dexterous Operations</heading>
                            </a><br>
                                <strong>Jyun-Ting Song</strong>, Jungeun Kim, Jinkun Cao, Yu Lei, Takuma Yagi, Kris Kitani
                                <br>
                                <em>3D Vision (3DV)</em>, 2026
                                <br>
                                <br>
                                A large-scale whole-body human dataset for dexterous operations with finger contact annotations  
                                <br>    
                            </p>
    
                            <div class="paper" id="contact4d">
                                <a href="https://arxiv.org/abs/2410.20294">paper</a> |
                                <a href="javascript:toggleblock(&#39;contact4d_abs&#39;)">abstract</a> |
                                <a href="https://jyuntins.github.io/harmony4d/">project</a> |
                                <a href="https://huggingface.co/datasets/Jyun-Ting/Harmony4D/tree/main">dataset</a> |
                                <!-- <a shape="rect" href="javascript:togglebib(&#39;harmony4d&#39;)" class="togglebib">bibtex</a>  -->
                                <a shape="rect" href="javascript:togglebib(&#39;contact4d&#39;)" class="togglebib">bibtex</a>

                                <p align="justify"><i id="contact4d_abs" style="display: none;"> 
                                  Understanding how humans interact with objects is key to building robust human-centric artificial intelligence. 
                                  However, this area remains relatively unexplored due to the lack of large-scale datasets. 
                                  Recent datasets focusing on this issue mainly consist of activities captured entirely in controlled lab environments, 
                                  and contact annotations are mostly estimated using threshold clips. 
                                  We introduce Contact4D, a multi-view video dataset for human-object interaction that provides detailed body poses and accurate contact annotations. 
                                  We use a flexible multi-view capture system to record individuals performing furniture assembly tasks and provide annotations for 
                                  human detection, tracking, 2D/3D pose estimation, and ground-truth contact. 
                                  Additionally, we propose a novel processing pipeline to extract accurate hand poses even when they are severely occluded. 
                                  Contact4D consists of 2M images captured from 19 synchronized cameras across 350 video sequences, 
                                  spanning diverse environments, varioius furniture types, and unique subjects. 
                                  We evaluate existing methods for human pose estimation and human-centric contact estimation, 
                                  demonstrating their inability to generalize to our dataset. 
                                  Lastly, we fine-tune a pretrained MultiHMR model on Contact4D and observe an improved performance of 56.6% body MPJPE and 26.4% hand MPJPE in scenarios under severe self-occlusion and object occlusion. 
                                  </i>
                                </p>
    
                                <!-- <div style="white-space: pre-wrap; display: none;" class="bib"> -->
                                <div id="contact4d_bib" style="white-space: pre-wrap; display: none;" class="bib">
@inproceedings{song2024contact4d,
title     = {Contact4D: A Video Dataset for Whole-body Human Motion and Finger Contact in Dexterous Operations},
author    = {Song, Jyun-Ting and Kim, Jungeun and Cao, Jinkun and Lei, Yu and Yagi, Takuma and Kitani, Kris},
booktitle = {3D Vision (3DV)},
year      = {2026}
}
                                </div>
                            </div>
                        </td>
                    </tr>




                    <tr>
                        <td style="padding:20px;width:20%;vertical-align:middle">
                          <video  width=100% muted autoplay loop>
                            <source src="images/harmony4d.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                          </video>
                        </td>

                        <td width="58%" valign="top">
                            <p><a href="https://jyuntins.github.io/harmony4d" id="harmony4d_title">
                                <heading>Harmony4D: A Video Dataset for In-The-Wild Close Human Interactions</heading>
                            </a><br>
                                Rawal Khirodkar*, <strong>Jyun-Ting Song*</strong>, Jinkun Cao, Zhengyi Luo, Kris Kitani
                                <br>
                                <em>Neural Information Processing Systems (NeurIPS)</em>, 2024
                                <br>
                                <br>
                                A large-scale multihuman dataset captured in in-the-wild environments, featuring diverse dynamic activities  
                                <br>    
                            </p>
    
                            <div class="paper" id="harmony4d">
                                <a href="https://arxiv.org/abs/2410.20294">paper</a> |
                                <a href="javascript:toggleblock(&#39;harmony4d_abs&#39;)">abstract</a> |
                                <a href="https://jyuntins.github.io/harmony4d/">project</a> |
                                <a href="https://huggingface.co/datasets/Jyun-Ting/Harmony4D/tree/main">dataset</a> |
                                <!-- <a shape="rect" href="javascript:togglebib(&#39;harmony4d&#39;)" class="togglebib">bibtex</a>  -->
                                <a shape="rect" href="javascript:togglebib(&#39;harmony4d&#39;)" class="togglebib">bibtex</a>

                                <p align="justify"><i id="harmony4d_abs" style="display: none;"> 
                                  Understanding how humans interact with each other is key to building realistic multi-human virtual reality systems. 
                                  This area remains relatively unexplored due to the lack of large-scale datasets. Recent datasets focusing on this 
                                  issue mainly consist of activities captured entirely in controlled indoor environments with choreographed actions, 
                                  significantly affecting their diversity. To address this, we introduce Harmony4D, a multi-view video dataset for 
                                  human-human interaction featuring in-the-wild activities such as wrestling, dancing, MMA, and more. We use a flexible 
                                  multi-view capture system to record these dynamic activities and provide annotations for human detection, tracking, 
                                  2D/3D pose estimation, and mesh recovery for closely interacting subjects. We propose a novel markerless algorithm 
                                  to track 3D human poses in severe occlusion and close interaction to obtain our annotations with minimal manual 
                                  intervention. Harmony4D consists of 1.66 million images and 3.32 million human instances from more than 20 
                                  synchronized cameras with 208 video sequences spanning diverse environments and 24 unique subjects. We rigorously 
                                  evaluate existing stateof-the-art methods for mesh recovery and highlight their significant limitations in modeling 
                                  close interaction scenarios. Additionally, we fine-tune a pre-trained HMR2.0 model on Harmony4D and demonstrate an 
                                  improved performance of 54.8% PVE in scenes with severe occlusion and contact.</i>
                                </p>
    
                                <!-- <div style="white-space: pre-wrap; display: none;" class="bib"> -->
                                <div id="harmony4d_bib" style="white-space: pre-wrap; display: none;" class="bib">
@inproceedings{khirodkar2024harmony4d,
title     = {Harmony4D: A Video Dataset for In-The-Wild Close Human Interactions},
author    = {Khirodkar, Rawal and Song, Jyun-Ting and Cao, Jinkun and Luo, Zhengyi and Kitani, Kris},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year      = {2024}
}
                                </div>
                            </div>
                        </td>
                    </tr>
    
                    <tr>
                        <td style="padding:20px;width:20%;vertical-align:middle">
                          <video  width=100% muted autoplay loop>
                            <source src="images/balance_board.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                          </video>
                        </td>
                        <td width="58%" valign="top">
                            <p><a href="https://github.com/NTNU-ERC/Robinion-Balance-Board-PPO" id="balance board">
                                <heading>Reinforcement Learning and Action Space Shaping for a Humanoid Agent in a Highly Dynamic Environment</heading>
                            </a><br>
                                <strong>Jyun-Ting Song</strong>, Guilherme Christmann, Jaesik Jeong, Jacky Baltes<br>
                                <em>Springer's Studies in Computational Intelligence</em>, 2023
                                <br>
                                <br>
                                RL algorithm structure based on Proximal Policy Optimization (PPO) to train a humanoid agent 
                                to play a balance board in Isaac Gym                                
                                <br>
    
                            </p>
    
                            <div class="paper" id="balanceboard">
                                <a href="https://link.springer.com/chapter/10.1007/978-3-031-26135-0_4">paper</a> |
                                <a href="javascript:toggleblock(&#39;balanceboard_abs&#39;)">abstract</a> |
                                <a href="https://github.com/NTNU-ERC/Robinion-Balance-Board-PPO">project</a> |
                                <a shape="rect" href="javascript:togglebib(&#39;balanceboard&#39;)" class="togglebib">bibtex</a> 
    
    
                                <p align="justify"><i id="balanceboard_abs" style="display: none;"> 
                                  Reinforcement Learning (RL) is a powerful tool and has been increasingly used in continuous control tasks such 
                                  as locomotion and balancing in robotics. In this paper, we tackle a balancing task in a highly dynamic environment, 
                                  using a humanoid agent and a balancing board. This task requires continuous actuation in order for the agent to 
                                  stay in a balanced state. We developed an implementation using a state-of-theart RL algorithm and simulator that 
                                  can achieve successful balancing in under 40 minutes of real-time with a single GPU. We sought to examine the impact 
                                  of action space shaping in sample efficiency and designed 6 distinct control modes. Our constrained parallel control 
                                  modes outperform a naive baseline in both sample efficiency and variance to the starting seed. The best performing 
                                  control mode (PLS-R) is around 33% more sample efficient than the second-best, requiring 70 million fewer training 
                                  timesteps to reach comparable performance.
                                  </i>
                                </p>
    
                                <div id="balanceboard_bib" style="white-space: pre-wrap; display: none;" class="bib">
@incollection{song2023reinforcement,
title     = {Reinforcement Learning and Action Space Shaping for a Humanoid Agent in a Highly Dynamic Environment},
author    = {Song, Jyun-Ting and Christmann, Guilherme and Jeong, Jaesik and Baltes, Jacky},
booktitle = {Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing 2022-Winter},
pages     = {29--42},
year      = {2023},
publisher = {Springer}
}
                                </div>
                            </div>
                        </td>
                    </tr>
    
                    <tr>
                        <td style="padding:20px;width:20%;vertical-align:middle">
                          <video  width=100% muted autoplay loop>
                            <source src="images/corsmal.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                          </video>
                        </td>
                        <td width="58%" valign="top">
                            <p><a href="https://corsmal.eecs.qmul.ac.uk/benchmark.html" id="corsmal_title">
                                <heading>The CORSMAL benchmark for the prediction of the properties of containers</heading>
                            </a><br>
                                Alessio Xompero, et al.<br>
                                <em>IEEE Access</em>, 2022
                                <br>
                                <br>
                                Estimate mass, type, and fill level of containers using multimodal dataset (visual, audio)
                                <br>
    
                            </p>
    
                            <div class="paper" id="corsmal">
                                <a href="https://ieeexplore.ieee.org/abstract/document/9756023">paper</a> |
                                <a href="javascript:toggleblock(&#39;corsmal_abs&#39;)">abstract</a> |
                                <a href="https://corsmal.eecs.qmul.ac.uk/benchmark.html">project</a> |
                                <a shape="rect" href="javascript:togglebib(&#39;corsmal&#39;)" class="togglebib">bibtex</a> |
    
    
                                <p align="justify"><i id="corsmal_abs" style="display: none;"> 
                                  The contactless estimation of the weight of a container and the amount of its content manipulated by a person are 
                                  key pre-requisites for safe human-to-robot handovers. However, opaqueness and transparencies of the container and 
                                  the content, and variability of materials, shapes, and sizes, make this estimation difficult. In this paper, 
                                  we present a range of methods and an open framework to benchmark acoustic and visual perception for the estimation 
                                  of the capacity of a container, and the type, mass, and amount of its content. The framework includes a dataset, 
                                  specific tasks and performance measures. We conduct an in-depth comparative analysis of methods that used this 
                                  framework and audio-only or vision-only baselines designed from related works. Based on this analysis, we can conclude 
                                  that audio-only and audio-visual classifiers are suitable for the estimation of the type and amount of the content 
                                  using different types of convolutional neural networks, combined with either recurrent neural networks or a majority 
                                  voting strategy, whereas computer vision methods are suitable to determine the capacity of the container using 
                                  regression and geometric approaches. Classifying the content type and level using only audio achieves a weighted average 
                                  F1-score up to 81% and 97%, respectively. Estimating the container capacity with vision-only approaches and estimating 
                                  the filling mass with audio-visual multi-stage approaches reach up to 65% weighted average capacity and mass scores. 
                                  These results show that there is still room for improvement on the design of new methods. These new methods can be ranked 
                                  and compared on the individual leaderboards provided by our open framework.
                                </i>
                                </p>
    
                                <div id="corsmal_bib" style="white-space: pre-wrap; display: none;" class="bib">
@article{xompero2022corsmal,
title     = {The CORSMAL benchmark for the prediction of the properties of containers},
author    = {Xompero, Alessio and Donaher, Santiago and Iashin, Vladimir and Palermo, Francesca and Solak, G{\"o}khan and Coppola, Claudio and Ishikawa, Reina and Nagao, Yuichi and Hachiuma, Ryo and Liu, Qi and others},
journal   = {IEEE Access},
volume    = {10},
pages     = {41388--41402},
year      = {2022},
publisher = {IEEE}
}
                                </div>
                            </div>
                        </td>
                    </tr>

                  </tbody>
                </table>
    
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                    <tr>
                        <td>
                            <br>
                            <p align="right"><font size="2">
                                <a href="https://www.cs.berkeley.edu/~barron/">adapted from Jon Barron's awesome webpage</a>
                            </font></p>
    
                        </td>
                    </tr>
                    </tbody>
                </table>
    
                <script xml:space="preserve" language="JavaScript">
      hideallbibs();
    </script>
                <script xml:space="preserve" language="JavaScript">
      hideblock('pa13_abs');
    </script>
                <script xml:space="preserve" language="JavaScript">
      hideblock('cuboid_abs');
    </script>
                <script xml:space="preserve" language="JavaScript">
      hideblock('mftcn_abs');
    </script>
                <script xml:space="preserve" language="JavaScript">
      hideblock('temporal_abs');
    </script>
    
    
  </body>
</html>
















  <!-- <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jyun-Ting Song
                </p>
                <p>
                  I am a second-year master's student at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, 
                  advised by Prof. <a href="https://kriskitani.github.io/">Kris Kitani</a>.
                  Previously, I earned my B.S. in Electrical Engineering from <a href="https://en.ntnu.edu.tw/">National Taiwan Normal University</a>, 
                  where I was advised by Prof. <a href="https://sites.google.com/site/jackybaltes/home">Jacky Baltes</a>.
                  <br>
                  <br>
                  My research interest lies in the intersection of Computer Vision, Machine Learning and Robotics.
                  Currently, I am working on human pose estimation and human-object contact detection.
                  I love humans, and I am interested in understanding them. I hope my reseach can contribute to this direction.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jyuntins@andrew.cmu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Jyun-Ting_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=N1dz8cAAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jyuntins/">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/jyun-ting-song-699b91222/">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/jyuntins.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/jyuntins.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          Harmony4D
          <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
                <source src="images/harmony4d.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src="images/harmony4d.jpg" width=100%>
              </div>
              <script type="text/javascript">
                function smerf_start() {
                  document.getElementById('smerf_image').style.opacity = "1";
                }

                function smerf_stop() {
                  document.getElementById('smerf_image').style.opacity = "0";
                }
                smerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/NTNU-ERC/Robinion-Balance-Board-PPO">
                <span class="papertitle">Harmony4D: A Video Dataset for In-The-Wild Close Human Interactions</span>
              </a>
              <br>
              Rawal Khirodkar*, <strong>Jyun-Ting Song*</strong>, Jinkun Cao, Zhengyi Luo, Kris Kitani
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2024
              <br>
              <a href="https://jyuntins.github.io/harmony4d">project page</a>
              /
              <a href="https://github.com/jyuntins/harmony4d">code</a>
              /
              <a href="https://arxiv.org/abs/2410.20294">paper</a>
              /
              <a href="https://huggingface.co/datasets/Jyun-Ting/Harmony4D/tree/main">data</a>
              <p>
                A large-scale multihuman dataset captured in in-the-wild environments, featuring diverse dynamic activities
              </p>
            </td>
          </tr>

          Balancing Board
          <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
                <source src="images/balance_board.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src="images/balance_board.jpg" width=100%>
              </div>
              <script type="text/javascript">
                function smerf_start() {
                  document.getElementById('smerf_image').style.opacity = "1";
                }

                function smerf_stop() {
                  document.getElementById('smerf_image').style.opacity = "0";
                }
                smerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/NTNU-ERC/Robinion-Balance-Board-PPO">
                <span class="papertitle">Reinforcement Learning and Action Space Shaping for a Humanoid Agent in a Highly Dynamic Environment</span>
              </a>
              <br>
              <strong>Jyun-Ting Song</strong>, Guilherme Christmann, Jaesik Jeong, Jacky Baltes
              <br>
              <em>Springer's Studies in Computational Intelligence</em>, 2023
              <br>
              <a href="https://github.com/NTNU-ERC/Robinion-Balance-Board-PPO">project page</a>
              /
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-26135-0_4">paper</a>
              <p>
                RL algorithm structure based on Proximal Policy Optimization (PPO) to train a humanoid agent 
                to play a balance board in Isaac Gym
              </p>
            </td>
          </tr>


          CORSMAL
          <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
                <source src="images/corsmal.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src="images/corsmal.png" width=100%>
              </div>
              <script type="text/javascript">
                function smerf_start() {
                  document.getElementById('smerf_image').style.opacity = "1";
                }

                function smerf_stop() {
                  document.getElementById('smerf_image').style.opacity = "0";
                }
                smerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/NTNU-ERC/Robinion-Balance-Board-PPO">
                <span class="papertitle">The CORSMAL Benchmark for the Prediction of the Properties of Containers</span>
              </a>
              <br>
              Alessio Xompero, et al.
              <br>
              <em>IEEE Access</em>, 2022
              <br>
              <a href="https://corsmal.eecs.qmul.ac.uk/benchmark.html">project page</a>
              /
              <a href="https://ieeexplore.ieee.org/document/9756023">paper</a>
              /
              <a href="https://github.com/guichristmann/CORSMAL-Challenge-2020-Submission">code</a>
              <p>
                Estimate mass, type, and fill level of containers using a multimodal dataset (visual, audio)
              </p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html> -->
